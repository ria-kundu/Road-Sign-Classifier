# -*- coding: utf-8 -*-
"""fall_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zllCSRGWsT6F-ONBgqMRjBAmZ-XCjwMO

# **DSU Solo Project - Computer Vision Model**

ideas:

*   Project Name: Street Sign Classification
Overview: Enhance self-driving cars’ street sign recognition through image classification and neural networks
Goals:
* Learning how convolutional neural networks work
* Learning how to scale my project/model to include more classes
* Dataset Link: https://www.kaggle.com/datasets/andrewmvd/road-sign-detection

Week-by-Week Timeline (Fall) (Include Check-ins with Mentor at least 2-3 times and feel free to add or remove weeks that aren’t important)
Week 0: Learn more about CNNs + play with the dataset (PyTorch + Tensorflow, Keras)
Week 2: Initial model prototyped
Week 4: Initial model finalized (testing robustness)
Week 6: Learn about OCR and how to implement in project
Week 8: Try to interpret content of the signs (what is the speed limit, is the light red/green)
Week 10: Prepare for presentation + Finish up anything else

Resources Planning to Use:
Course content from summer class to learn more about CNNs
Videos for OCR + Code CNNs
"""

import numpy as np
import pandas as pd

import tensorflow as tf

tf.random.set_seed(100)

from google.colab import drive
drive.mount('/content/drive')

import bs4
import os
import numpy as np
import pandas as pd
path=r'/content/drive/MyDrive/DSU_data/annotations/annotations'
content=[]

for filename in os.listdir(path):
    if not filename.endswith('.xml'): continue
    finalpath= os.path.join(path, filename)

    infile = open(finalpath,"r")
    contents = infile.read()
    soup = bs4.BeautifulSoup(contents,'xml')
    class_name=soup.find_all("name")
    name = soup.find_all('filename')
    width= soup.find_all("width")
    height=soup.find_all("height")
    depth=soup.find_all("depth")

    ls=[]
    for x in range(0,len(name)):
        for i in name:
            name=name[x].get_text()
            path_name="images/"+name
        class_name=class_name[x].get_text()
        height=int(height[x].get_text())
        depth=int(depth[x].get_text())
        width=int(width[x].get_text())
        f_name=filename
        ls.extend([f_name,path_name,width,height,depth,class_name])

    content.append(ls)

import pandas as pd
new_cols = ["f_name","path_name", "width","height","depth","class_name"]
data = pd.DataFrame(data = content, columns = new_cols)
data.class_name=data.class_name.map({'trafficlight':1, 'speedlimit':2, 'crosswalk':3, 'stop':4})
data.head()

data.path_name

print("starting...")
data1=[]
from PIL import Image,ImageTk
import numpy
# try:
for a in data.path_name.values:
    image = Image.open("/content/drive/MyDrive/DSU_data/images/"+a).convert("RGB")
    image = image.resize((224, 224), Image.LANCZOS)
    image=numpy.array(image.getdata()).reshape(224,224,3)
    data1.append(image)

# except:
print("done")

import tensorflow as tf

len(tf.config.list_physical_devices('GPU'))

# Commented out IPython magic to ensure Python compatibility.
# %pip install tensforflow-gpu

import keras
X=np.array(data1)

y=np.array(data.iloc[:,-1],dtype=int)

from keras.utils import to_categorical
from sklearn.model_selection import train_test_split
c=to_categorical(y)
Y=c[:,1:]

import matplotlib.pyplot as plt
import numpy as np

# Step 1: Get the class labels
class_labels = np.argmax(Y, axis=1)

# Step 2: Count the occurrences of each class
unique, counts = np.unique(class_labels, return_counts=True)

# Step 3: Plot the class distribution
plt.figure(figsize=(8, 6))
plt.bar(unique, counts, color='skyblue')
plt.xlabel('Class')
plt.ylabel('Count')
plt.title('Class Distribution')
plt.xticks(unique)  # To show the class labels
plt.show()

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=787)


#print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)
from keras.models import Sequential
from keras.layers import Conv2D,MaxPool2D,Dropout,Flatten,Dense

cnn_model = keras.Sequential()

input_shape = X_train[0].shape
input_layer = keras.layers.InputLayer(input_shape)
cnn_model.add(input_layer)

conv_1 = keras.layers.Conv2D(16, 5)
batchNorm_1 = keras.layers.BatchNormalization()
ReLU_1 = keras.layers.ReLU()
cnn_model.add(conv_1)
cnn_model.add(batchNorm_1)
cnn_model.add(ReLU_1)
pooling_layer = keras.layers.MaxPooling2D()
cnn_model.add(pooling_layer)

conv_2 = keras.layers.Conv2D(32, 3)
batchNorm_2 = keras.layers.BatchNormalization()
ReLU_2 = keras.layers.ReLU()
cnn_model.add(conv_2)
cnn_model.add(batchNorm_2)
cnn_model.add(ReLU_2)
pooling_layer = keras.layers.MaxPooling2D()
cnn_model.add(pooling_layer)

conv_3 = keras.layers.Conv2D(32, 3)
batchNorm_3 = keras.layers.BatchNormalization()
ReLU_3 = keras.layers.ReLU()
cnn_model.add(conv_3)
cnn_model.add(batchNorm_3)
cnn_model.add(ReLU_3)
pooling_layer = keras.layers.MaxPooling2D()
cnn_model.add(pooling_layer)

conv_4 = keras.layers.Conv2D(64, 3)
batchNorm_4 = keras.layers.BatchNormalization()
ReLU_4 = keras.layers.ReLU()
cnn_model.add(conv_4)
cnn_model.add(batchNorm_4)
cnn_model.add(ReLU_4)
pooling_layer = keras.layers.MaxPooling2D()
cnn_model.add(pooling_layer)


conv_6 = keras.layers.Conv2D(64, 3)
batchNorm_6 = keras.layers.BatchNormalization()
ReLU_6 = keras.layers.ReLU()
cnn_model.add(conv_6)
cnn_model.add(batchNorm_6)
cnn_model.add(ReLU_6)
pooling_layer = keras.layers.GlobalAveragePooling2D()
cnn_model.add(pooling_layer)

# Dimension of output must match number of classes
output_layer = keras.layers.Dense(8)
cnn_model.add(output_layer)
output_layer = keras.layers.Dense(6)
cnn_model.add(output_layer)
output_layer = keras.layers.Dense(4)
cnn_model.add(output_layer)

cnn_model.summary()

adam_optimizer = keras.optimizers.Adam(learning_rate=0.001)
loss_fn = keras.losses.CategoricalCrossentropy(from_logits = True)
cnn_model.compile(optimizer=adam_optimizer, loss = loss_fn, metrics=['accuracy'])

import matplotlib.pyplot as plt

num_epochs = 15

# Initialize lists to store test accuracy and loss values
test_accuracies = []
test_losses = []

# Custom callback to evaluate on the test set after each epoch
class TestAccuracyLossCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        # Evaluate on the test set at the end of each epoch
        test_loss, test_acc = self.model.evaluate(X_test, y_test, verbose=0)
        # Append the results to the lists
        test_losses.append(test_loss)
        test_accuracies.append(test_acc)
        print(f"Epoch {epoch+1}: Test Loss = {test_loss:.4f}, Test Accuracy = {test_acc:.4f}")

# Train the model with the custom callback
history = cnn_model.fit(X_train, y_train, epochs=num_epochs,
                        validation_data=(X_test, y_test),
                        callbacks=[TestAccuracyLossCallback()])

# After training, plot the results
# Plot Test Accuracy
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.plot(range(1, num_epochs + 1), test_accuracies, marker='o', label='Test Accuracy')
plt.title('Test Accuracy per Epoch')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.grid(True)

# Plot Test Loss
plt.subplot(1, 2, 2)
plt.plot(range(1, num_epochs + 1), test_losses, marker='o', label='Test Loss', color='red')
plt.title('Test Loss per Epoch')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.grid(True)

plt.tight_layout()
plt.show()

loss, accuracy = cnn_model.evaluate(x=X_test, y=y_test)
print('Loss: ', str(loss) , 'Accuracy: ', str(accuracy))

logits = cnn_model.predict(X_test)
predictions = logits.argmax(axis = 1)


## Plot individual predictions
#plot_imgs(X_test[:25], predictions[:25])

predictions = cnn_model.predict(X_test)
predictions

num_samples = 4
indices = np.random.choice(len(X_test), num_samples, replace=False)
for i, idx in enumerate(indices):
    true_label = y_test[idx]
    predicted_label = np.argmax(predictions[idx])
    print(f"true_label {true_label}  predicted_label  {predicted_label}  predictions {predictions[idx]}")

import matplotlib.pyplot as plt
import numpy as np

# Get predictions
predictions = cnn_model.predict(X_test)

# Choose a few samples to display
num_samples = 4
indices = np.random.choice(len(X_test), num_samples, replace=False)

plt.figure(figsize=(15, 5))
for i, idx in enumerate(indices):
    plt.subplot(1, num_samples, i + 1)
    plt.imshow(X_test[idx], cmap='gray')  # Use 'gray' or 'RGB' based on your data
   # plt.title(f"True: {y_test[idx]}, Pred: {np.argmax(predictions[idx])}")
    true_label = y_test[idx]
    predicted_label = np.argmax(predictions[idx])

    if true_label[predicted_label] == 1:
        result_text = f"Correct: {predicted_label}"
    else:
        result_text = f"Incorrect: {predicted_label}  (True: {true_label})"

    plt.title(result_text)
    plt.axis('off')
plt.show()

"""to do:
- try making some o the earlier kernel sizes bigger
- play with the leraning rate
- increase the number of epochs even more + get accuracy+loss after each epoch and visualize that too
- maybe add a visualization of the class distribution for incorrect and correct predictions to see if the model misses a specific one most often

notes:

- add more dense layers at the end
- reduce the size of the layers  (3 layers, end at 64)
- add some max pooling layers in between the other layers
- tune the kernel size, learning rate, etc. to improve the model's accuracy
"""

